'''1. For test scores in .csv file, run gradient descent algorithm to find out
value of m, b and appropriate learning rate

2. On each iteration, compare previous cost with current cost. Stop
when costs are similar (use math.isclose function with le-20 thresold)

3. Now using sklearn.linear_model find coefficient (i.e. m) and
intercept (i.e b). Compare them with m, b generated by your gradient
descent algorithm'''

import numpy as np
import matplotlib.pyplot as plt
import sklearn.linear_model as sklm
import pandas as pd
import math

d = pd.read_csv('3_gradient_decent/test_scores.csv')
X = d.math.values
y = d.cs.values

def initialize_parameters(X):
    """
    Initialize parameters for gradient descent.
    """
    n = len(y)
    theta1 = 0
    theta2 = 0
    return theta1, theta2

def predict(X, theta1, theta2):
    """
    Predict the output using the linear model.
    """
    return theta1 + theta2 * X

def compute_cost(X, y, theta1, theta2):
    """
    Compute the cost function for linear regression.
    """
    m = len(y)
    predictions = predict(X, theta1, theta2)
    cost = (1 / (m)) * np.sum((predictions - y) ** 2)
    return cost

def gradient_descent(X, y, theta1, theta2, learning_rate, iterations):
    """
    Perform gradient descent to learn theta1 and theta2.
    """
    m = len(y)
    cost_history = []

    for i in range(iterations):
        predictions = predict(X, theta1, theta2)
        error = predictions - y
        
        # Compute gradients
        theta1_gradient = (-1 / m) * np.sum(error)
        theta2_gradient = (-1 / m) * np.sum(error * X)

        # Update parameters
        theta1 += learning_rate * theta1_gradient
        theta2 += learning_rate * theta2_gradient

        # Save the cost for plotting
        cost_history.append(compute_cost(X, y, theta1, theta2))
        # Check for convergence
        if i > 0 and math.isclose(cost_history[i], cost_history[i-1], rel_tol=1e-20):
            print(f"Convergence reached at iteration {i}")
            break
        # Print progress every 1000 iterations
        if i % 1000 == 0:
            print(f"Iteration {i}: Cost = {cost_history[-1]}, theta1 = {theta1}, theta2 = {theta2}")

    return theta1, theta2, cost_history

def main():
    # Initialize parameters
    theta1, theta2 = initialize_parameters(X)
    
    # Set hyperparameters
    learning_rate = 0.0001
    iterations = 100000
    
    # Run gradient descent
    theta1, theta2, cost_history = gradient_descent(X, y, theta1, theta2, learning_rate, iterations)
    # Print results
    print(f"Gradient Descent Results: theta1 (intercept) = {theta1}, theta2 (slope) = {theta2}")
    #scikit-learn linear regression for comparison
    model = sklm.LinearRegression()
    model.fit(X.reshape(-1, 1), y)
    sklearn_theta1 = model.intercept_
    sklearn_theta2 = model.coef_[0]
    print(f"Scikit-learn Results: theta1 (intercept) = {sklearn_theta1}, theta2 (slope) = {sklearn_theta2}")
    # Compare results
    print(f"difference in slope: {abs(theta2 - sklearn_theta2)}")
    print(f"difference in intercept: {abs(theta1 - sklearn_theta1)}")
    
    
    # Plotting the cost history
    plt.plot(cost_history)
    plt.xlabel('Iterations')
    plt.ylabel('Cost')
    plt.title('Cost History')
    plt.show()
    # Plotting the data points and the fitted line
    plt.scatter(X, y, color='blue', label='Data Points')
    plt.plot(X, predict(X, theta1, theta2), color='red', label='Gradient Descent Fit')
    plt.plot(X, model.predict(X.reshape(-1, 1)), color='green', label='Scikit-learn Fit', alpha=0.5)
    plt.xlabel('Math Scores')
    plt.ylabel('CS Scores')
    plt.title('Gradient Descent Linear Regression Fit')
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()